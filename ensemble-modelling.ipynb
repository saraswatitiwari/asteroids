{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d991075f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.016632,
     "end_time": "2022-02-27T16:36:24.508003",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.491371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Models\n",
    "\n",
    "When we want to purchase a new car, we'll walk up to the first car shop and purchase one based on the advice of the dealer? It’s highly unlikely.\n",
    "\n",
    "We would likely browser a few web portals where people have posted their reviews and compare different car models, checking for their features and prices. We will also probably ask our friends and colleagues for their opinion. In short, we wouldn’t directly reach a conclusion, but will instead make a decision considering the opinions of other people as well.\n",
    "\n",
    "Ensemble models in machine learning operate on a similar idea. They combine the decisions from multiple models to improve the overall performance. This can be achieved in various ways, which we will discover in this blog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2213cc",
   "metadata": {
    "papermill": {
     "duration": 0.015079,
     "end_time": "2022-02-27T16:36:24.538927",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.523848",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s understand the concept of ***ensemble learning*** with an example. Suppose I'm a movie director and i'm creating a short movie on a very important and interesting topic. Now, i want to take preliminary feedback (ratings) on the movie before making it public. What are the possible ways by which i can do that?\n",
    "\n",
    "A: I'm asking one of my friends to rate the movie for me.\n",
    "Now it’s entirely possible that the person i have chosen loves me very much and doesn’t want to break my heart by providing a 1-star rating to the horrible work i have created.\n",
    "\n",
    "B: Another way could be by asking 5 colleagues of mine to rate the movie.\n",
    "This should provide a better idea of the movie. This method may provide honest ratings for our movie. But a problem still exists. These 5 people may not be “Subject Matter Experts” on the topic of our movie. Sure, they might understand the cinematography, the shots, or the audio, but at the same time may not be the best judges of dark humour.\n",
    "\n",
    "C: How about asking 50 people to rate the movie?\n",
    "Some of which can be our friends, some of them can be our colleagues and some may even be total strangers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972c4e4",
   "metadata": {
    "papermill": {
     "duration": 0.015307,
     "end_time": "2022-02-27T16:36:24.570533",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.555226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The responses, in this case, would be more generalized and diversified since now we have people with different sets of skills. And as it turns out – this is a better approach to get honest ratings than the previous cases we saw.\n",
    "\n",
    "With these examples, we can infer that a diverse group of people are likely to make better decisions as compared to individuals. Similar is true for a diverse set of models in comparison to single models. This diversification in Machine Learning is achieved by a technique called **Ensemble Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24d129",
   "metadata": {
    "papermill": {
     "duration": 0.015699,
     "end_time": "2022-02-27T16:36:24.601699",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.586000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Ensemble Techniques\n",
    "\n",
    "In this section, we will look at a few simple but powerful techniques, namely:\n",
    "\n",
    "1. Max Voting\n",
    "2. Averaging \n",
    "3. Weighted Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be4724",
   "metadata": {
    "papermill": {
     "duration": 0.015352,
     "end_time": "2022-02-27T16:36:24.632728",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.617376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Max Voting\n",
    "The max voting method is generally used for classification problems. In this technique, multiple models are used to make predictions for each data point. The predictions by each model are considered as a ‘vote’. The predictions which we get from the majority of the models are used as the final prediction.\n",
    "\n",
    "For example, when we asked 5 of our colleagues to rate our movie (out of 5); we’ll assume three of them rated it as 4 while two of them gave it a 5. Since the majority gave a rating of 4, the final rating will be taken as 4. We can consider this as taking the mode of all the predictions.\n",
    "\n",
    "The result of max voting would be something like this:\n",
    "```\n",
    "Colleague 1 \t     Colleague 2\t    Colleague 3 \tColleague 4 \tColleague 5 \tFinal rating\n",
    "   5\t                   4\t               5\t           4\t        4\t            4\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c962c",
   "metadata": {
    "papermill": {
     "duration": 0.01521,
     "end_time": "2022-02-27T16:36:24.663482",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.648272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here x_train consists of independent variables in training data, y_train is the target variable for training data. The validation set is x_test (independent variables) and y_test (target variable).\n",
    "```\n",
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "```\n",
    "```\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "```\n",
    "```\n",
    "pred1=model1.predict(x_test)\n",
    "pred2=model2.predict(x_test)\n",
    "pred3=model3.predict(x_test)\n",
    "```\n",
    "```\n",
    "final_pred = np.array([])\n",
    "for i in range(0,len(x_test)):\n",
    "    final_pred = np.append(final_pred, mode([pred1[i], pred2[i], pred3[i]]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b062e1",
   "metadata": {
    "papermill": {
     "duration": 0.015604,
     "end_time": "2022-02-27T16:36:24.694739",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.679135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can use “VotingClassifier” module in sklearn as follows:\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "model1 = LogisticRegression(random_state=1)\n",
    "model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "model.fit(x_train,y_train)\n",
    "model.score(x_test,y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f71cda",
   "metadata": {
    "papermill": {
     "duration": 0.015437,
     "end_time": "2022-02-27T16:36:24.725922",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.710485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Averaging\n",
    "\n",
    "In this method, we take an average of predictions from all the models and use it to make the final prediction. Averaging can be used for making predictions in regression problems or while calculating probabilities for classification problems.\n",
    "\n",
    "For example, in the below case, the averaging method would take the average of all the values.\n",
    "\n",
    "i.e. (5+4+5+4+4)/5 = 4.4\n",
    "\n",
    "```\n",
    "Colleague 1 \tColleague 2 \tColleague 3 \tColleague 4 \tColleague 5 \tFinal rating\n",
    "       5\t           4\t           5\t           4\t           4\t        4.4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f96b46",
   "metadata": {
    "papermill": {
     "duration": 0.015863,
     "end_time": "2022-02-27T16:36:24.757476",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.741613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1+pred2+pred3)/3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32adae",
   "metadata": {
    "papermill": {
     "duration": 0.015639,
     "end_time": "2022-02-27T16:36:24.788836",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.773197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Weighted Average\n",
    "\n",
    "This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. \n",
    "\n",
    "For instance, if two of our colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.\n",
    "\n",
    "\n",
    "The result is calculated as [(5*0.23) + (4*0.23) + (5*0.18) + (4*0.18) + (4*0.18)] = 4.41.\n",
    "\n",
    "\n",
    "              Colleague 1   \t   Colleague 2  \t  Colleague 3   \t  Colleague 4   \t  Colleague 5   \t  Final rating\n",
    "```\n",
    "weight      \t0.23        \t    0.23        \t    0.18    \t    0.18        \t    0.18            \t\n",
    "rating      \t5           \t       4        \t       5    \t       4        \t       4            \t   4.41\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61817ce",
   "metadata": {
    "papermill": {
     "duration": 0.016214,
     "end_time": "2022-02-27T16:36:24.822505",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.806291",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "```\n",
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = KNeighborsClassifier()\n",
    "model3= LogisticRegression()\n",
    "\n",
    "model1.fit(x_train,y_train)\n",
    "model2.fit(x_train,y_train)\n",
    "model3.fit(x_train,y_train)\n",
    "\n",
    "pred1=model1.predict_proba(x_test)\n",
    "pred2=model2.predict_proba(x_test)\n",
    "pred3=model3.predict_proba(x_test)\n",
    "\n",
    "finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64215f",
   "metadata": {
    "papermill": {
     "duration": 0.015781,
     "end_time": "2022-02-27T16:36:24.854370",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.838589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Bagging\n",
    "\n",
    " The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result.\n",
    " \n",
    " If we create all the models on the same set of data and combine it, will it be useful?\n",
    " \n",
    " Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.\n",
    " \n",
    "1. Multiple subsets are created from the original dataset, selecting observations with replacement.\n",
    "2. A base model (weak model) is created on each of these subsets.\n",
    "3. The models run in parallel and are independent of each other.\n",
    "4. The final predictions are determined by combining the predictions from all the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195aa892",
   "metadata": {
    "papermill": {
     "duration": 0.017885,
     "end_time": "2022-02-27T16:36:24.888174",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.870289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Boosting\n",
    "\n",
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Let’s understand the way boosting works in the below steps.\n",
    "\n",
    "1. A subset is created from the original dataset.\n",
    "2. Initially, all data points are given equal weights.\n",
    "3. A base model is created on this subset.\n",
    "4. This model is used to make predictions on the whole dataset.\n",
    "5. Errors are calculated using the actual values and predicted values.\n",
    "6. The observations which are incorrectly predicted, are given higher weights.\n",
    "7. Another model is created and predictions are made on the dataset.\n",
    "8. Similarly, multiple models are created, each correcting the errors of the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b830e",
   "metadata": {
    "papermill": {
     "duration": 0.015503,
     "end_time": "2022-02-27T16:36:24.919988",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.904485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Stacking\n",
    "\n",
    "Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. \n",
    "\n",
    "This model is used for making predictions on the test set. \n",
    "\n",
    "1. The train set is split into 10 parts.\n",
    "2. A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.\n",
    "3. The base model (in this case, decision tree) is then fitted on the whole train dataset.\n",
    "4. Using this model, predictions are made on the test set.\n",
    "5. Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086ae4e",
   "metadata": {
    "papermill": {
     "duration": 0.015331,
     "end_time": "2022-02-27T16:36:24.950826",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.935495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Blending\n",
    "\n",
    "Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. \n",
    "\n",
    "1. The train set is split into training and validation sets.\n",
    "2. Model(s) are fitted on the training set.\n",
    "3. The predictions are made on the validation set and the test set.\n",
    "4. The validation set and its predictions are used as features to build a new model.\n",
    "5. This model is used to make final predictions on the test and meta-features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeccf4de",
   "metadata": {
    "papermill": {
     "duration": 0.015293,
     "end_time": "2022-02-27T16:36:24.981959",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.966666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Algorithms based on Bagging and Boosting\n",
    "\n",
    "Bagging and Boosting are two of the most commonly used techniques in machine learning.\n",
    "\n",
    "Following are the algorithms we will be focusing on:\n",
    "\n",
    "**Bagging algorithms**:\n",
    "\n",
    "- Bagging meta-estimator\n",
    "- Random forest\n",
    "\n",
    "**Boosting algorithms**:\n",
    "\n",
    "- AdaBoost\n",
    "- GBM\n",
    "- XGBM\n",
    "- Light GBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e3f30",
   "metadata": {
    "papermill": {
     "duration": 0.015376,
     "end_time": "2022-02-27T16:36:25.012872",
     "exception": false,
     "start_time": "2022-02-27T16:36:24.997496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Bagging meta-estimator \n",
    "\n",
    "Bagging meta-estimator is an ensembling algorithm that can be used for both classification (BaggingClassifier) and regression (BaggingRegressor) problems. It follows the typical bagging technique to make predictions. Following are the steps for the bagging meta-estimator algorithm:\n",
    "\n",
    "- Random subsets are created from the original dataset (Bootstrapping).\n",
    "- The subset of the dataset includes all features.\n",
    "- A user-specified base estimator is fitted on each of these smaller sets.\n",
    "- Predictions from each model are combined to get the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfc317",
   "metadata": {
    "papermill": {
     "duration": 0.016036,
     "end_time": "2022-02-27T16:36:25.044532",
     "exception": false,
     "start_time": "2022-02-27T16:36:25.028496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Random Forest\n",
    "\n",
    "Random Forest is another ensemble machine learning algorithm that follows the bagging technique. It is an extension of the bagging estimator algorithm. The base estimators in random forest are decision trees.\n",
    "\n",
    "This is what a random forest model does:\n",
    "\n",
    "- Random subsets are created from the original dataset (bootstrapping).\n",
    "- At each node in the decision tree, only a random set of features are considered to decide the best split.\n",
    "- A decision tree model is fitted on each of the subsets.\n",
    "- The final prediction is calculated by averaging the predictions from all decision trees.\n",
    "\n",
    "**Note**: The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81db446",
   "metadata": {
    "papermill": {
     "duration": 0.015221,
     "end_time": "2022-02-27T16:36:25.076167",
     "exception": false,
     "start_time": "2022-02-27T16:36:25.060946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### AdaBoost\n",
    "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling.\n",
    "\n",
    "Steps for performing the AdaBoost algorithm:\n",
    "\n",
    "1. Initially, all observations in the dataset are given equal weights.\n",
    "2. A model is built on a subset of data.\n",
    "3. Using this model, predictions are made on the whole dataset.\n",
    "4. Errors are calculated by comparing the predictions and actual values.\n",
    "5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n",
    "6. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n",
    "7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91b55f",
   "metadata": {
    "papermill": {
     "duration": 0.015289,
     "end_time": "2022-02-27T16:36:25.107163",
     "exception": false,
     "start_time": "2022-02-27T16:36:25.091874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Gradient Boosting (GBM)\n",
    "\n",
    "Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd1936",
   "metadata": {
    "papermill": {
     "duration": 0.015262,
     "end_time": "2022-02-27T16:36:25.137934",
     "exception": false,
     "start_time": "2022-02-27T16:36:25.122672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### XGBoost\n",
    "\n",
    "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as ‘regularized boosting‘ technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161bf6d",
   "metadata": {
    "papermill": {
     "duration": 0.01501,
     "end_time": "2022-02-27T16:36:25.168590",
     "exception": false,
     "start_time": "2022-02-27T16:36:25.153580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Light GBM\n",
    "\n",
    "Before discussing how Light GBM works, let’s first understand why we need this algorithm when we have so many others (like the ones we have seen above). Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset.\n",
    "\n",
    "LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bbd31",
   "metadata": {
    "papermill": {
     "duration": 0.015161,
     "end_time": "2022-02-27T16:36:25.199182",
     "exception": false,
     "start_time": "2022-02-27T16:36:25.184021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### CatBoost\n",
    "\n",
    "CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms.\n",
    "\n",
    "Handling categorical variables is a tedious process, especially when we have a large number of such variables. When our categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b77a35",
   "metadata": {
    "papermill": {
     "duration": 0.015187,
     "end_time": "2022-02-27T16:36:25.230199",
     "exception": false,
     "start_time": "2022-02-27T16:36:25.215012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Thank You !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.909377,
   "end_time": "2022-02-27T16:36:25.857649",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-27T16:36:13.948272",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
